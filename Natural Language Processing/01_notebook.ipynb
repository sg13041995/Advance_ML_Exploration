{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Book:**\n",
    "- Deep Learning with Python, Second Edition\n",
    "  - Book by François Chollet\n",
    "  - François Chollet is a French software engineer and artificial intelligence researcher currently working at Google. Chollet is the creator of the Keras deep-learning library, released in 2015, and a main contributor to the TensorFlow machine learning framework.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What modern NLP is about: using machine learning and large datasets to give computers the ability not to understand language, which is a more lofty goal, but to ingest a piece of language as input and return something useful, like predicting the following:\n",
    "- “What’s the topic of this text?” (text classification)\n",
    "- “Does this text contain abuse?” (content filtering)\n",
    "- “Does this text sound positive or negative?” (sentiment analysis)\n",
    "- “What should be the next word in this incomplete sentence?” (language modeling)\n",
    "- “How would you say this in German?” (translation)\n",
    "- “How would you summarize this article in one paragraph?” (summarization)\n",
    "\n",
    "etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pre-processing template**\n",
    "- First, you *standardize* the text to make it easier to process, such as by converting \n",
    "it to lowercase or removing punctuation.\n",
    "- You split the text into units (called tokens), such as characters, words, or groups\n",
    "of words. This is called *tokenization*.\n",
    "- You convert each such token into a numerical vector. This will usually involve\n",
    "first *indexing* all tokens present in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Type**\n",
    "There are two kinds of text-processing models: \n",
    "- Those that care about word order, called **sequence models**.\n",
    "- Those that treat input words as a set, discarding their original order, called **bag-of-words models**.\n",
    "\n",
    "If you’re building a sequence model, you’ll use word-level tokenization, and if you’re building a bag-of-words model, you’ll use N-gram tokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "vocabulary = {}\n",
    "\n",
    "for text in dataset:\n",
    "    text = standardize(text)\n",
    "    tokens = tokenize(text)\n",
    "    for token in tokens:\n",
    "        if token not in vocabulary:\n",
    "        vocabulary[token] = len(vocabulary)\n",
    "```\n",
    "\n",
    "You can then convert that integer into a vector encoding that can be processed by a neural network, like a one-hot vector.\n",
    "\n",
    "```\n",
    "def one_hot_encode_token(token):\n",
    "    vector = np.zeros((len(vocabulary),))\n",
    "    token_index = vocabulary[token]\n",
    "    vector[token_index] = 1\n",
    "    return vector\n",
    "```\n",
    "\n",
    "Note that at this step it’s common to restrict the vocabulary to only the top 20,000 or 30,000 most common words found in the training data. Any text dataset tends to feature an extremely large number of unique terms, most of which only show up once or\n",
    "twice—indexing those rare terms would result in an excessively large feature space, where most features would have almost no information content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OOV - Out of vocabulary**\n",
    "\n",
    "Now, there’s an important detail here that we shouldn’t overlook: when we look up a new token in our vocabulary index, it may not necessarily exist. \n",
    "\n",
    "Your training data may not have contained any instance of the word “cherimoya” (or maybe you excluded it from your index because it was too rare), so doing `token_index = vocabulary[\"cherimoya\"]` may result in a KeyError. \n",
    "\n",
    "To handle this, you should use an “out of vocabulary” index (abbreviated as OOV index)—a catch-all for any token that wasn’t in the index. \n",
    "\n",
    "It’s usually index 1: you’re actually doing `token_index = vocabulary.get(token, 1)`. \n",
    "\n",
    "When decoding a sequence of integers back into words, you’ll replace 1 with something like “[UNK]” (which you’d call an “OOV token”)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mask Token or Padding**\n",
    "\n",
    "There are two special tokens that you will commonly use: the OOV token (index 1), and the mask token (index 0). \n",
    "\n",
    "While the OOV token means “here was a word we did not recognize,” the mask token tells us “ignore me, I’m not a word.” You’d use it in particular to pad sequence data: because data batches need to be contiguous, all sequences in a batch of sequence data must have the same length, so shorter sequences should be padded to the length of the longest sequence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextVectorization layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "# Vectorizer class\n",
    "class Vectorizer:\n",
    "\n",
    "    # Standarization method\n",
    "    def standardize(self, text):\n",
    "        # lowercasing the text\n",
    "        text = text.lower()\n",
    "        # returning the text characters one by one if it does not belong to any punctuation\n",
    "        return \"\".join(char for char in text\n",
    "                       if char not in string.punctuation)\n",
    "    \n",
    "    # Tokenization method\n",
    "    def tokenize(self, text):\n",
    "        # Calling the standardization method\n",
    "        text = self.standardize(text)\n",
    "        # Tokenizing and returning the text\n",
    "        return text.split()\n",
    "    \n",
    "    # Create index-word vocabulary\n",
    "    def make_vocabulary(self, dataset):\n",
    "        # Masking and OOV indices\n",
    "        self.vocabulary = {\"\": 0, \"[UNK]\": 1}\n",
    "\n",
    "        # Iterating over each separate text (sentence, paragraph) in the dataset\n",
    "        for text in dataset:\n",
    "            # Standardize and Tokenize\n",
    "            text = self.standardize(text)\n",
    "            tokens = self.tokenize(text)\n",
    "\n",
    "            # Iterating over each token\n",
    "            for token in tokens:\n",
    "                # If token not in the vocabulary already\n",
    "                if token not in self.vocabulary:\n",
    "                    self.vocabulary[token] = len(self.vocabulary)\n",
    "\n",
    "        # Inversing (word-index) to (index-word)                   \n",
    "        self.inverse_vocabulary = dict(\n",
    "            (v, k) for k, v in self.vocabulary.items()\n",
    "            )\n",
    "\n",
    "    # Token to index\n",
    "    def encode(self, text):\n",
    "        text = self.standardize(text)\n",
    "        tokens = self.tokenize(text)\n",
    "        return [self.vocabulary.get(token, 1) for token in tokens]\n",
    "\n",
    "    # Index to token\n",
    "    def decode(self, int_sequence):\n",
    "        return \" \".join(\n",
    "        self.inverse_vocabulary.get(i, \"[UNK]\") for i in int_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizer object\n",
    "vectorizer = Vectorizer()\n",
    "\n",
    "# Dataset\n",
    "dataset = [\n",
    "    \"I write, erase, rewrite\",\n",
    "    \"Erase again, and then\",\n",
    "    \"A poppy blooms.\",\n",
    "    ]\n",
    "\n",
    "# Vocabulary created\n",
    "vectorizer.make_vocabulary(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 5, 7, 1, 5, 6]\n",
      "i write rewrite and [UNK] rewrite again\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
    "\n",
    "# Encode test sentence\n",
    "encoded_sentence = vectorizer.encode(test_sentence)\n",
    "print(encoded_sentence)\n",
    "\n",
    "# Decode encoded test sentence\n",
    "decoded_sentence = vectorizer.decode(encoded_sentence)\n",
    "print(decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Keras in-built class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Custom standardization function\n",
    "def custom_standardization_fn(string_tensor):\n",
    "    # Lowercasing\n",
    "    lowercase_string = tf.strings.lower(string_tensor)\n",
    "    \n",
    "    # Replace punctuations with blank character\n",
    "    return tf.strings.regex_replace(\n",
    "        lowercase_string, f\"[{re.escape(string.punctuation)}]\", \"\")\n",
    "\n",
    "# Custom split function\n",
    "def custom_split_fn(string_tensor):\n",
    "    return tf.strings.split(string_tensor)\n",
    "\n",
    "# Custom TextVectorizer\n",
    "text_vectorization = TextVectorization(\n",
    "    output_mode=\"int\",\n",
    "    standardize=custom_standardization_fn,\n",
    "    split=custom_split_fn,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = [\n",
    "\"I write, erase, rewrite\",\n",
    "\"Erase again, and then\",\n",
    "\"A poppy blooms.\",\n",
    "]\n",
    "\n",
    "# Creating vocabulary\n",
    "text_vectorization.adapt(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', 'erase', 'write', 'then', 'rewrite', 'poppy', 'i', 'blooms', 'and', 'again', 'a']\n"
     ]
    }
   ],
   "source": [
    "# Display vocab\n",
    "print(text_vectorization.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 7  3  5  9  1  5 10], shape=(7,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# Encode\n",
    "\n",
    "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
    "\n",
    "encoded_sentence = text_vectorization(test_sentence)\n",
    "print(encoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i write rewrite and [UNK] rewrite again\n"
     ]
    }
   ],
   "source": [
    "# Decode\n",
    "\n",
    "# Getting list of vocab words\n",
    "vocabulary = text_vectorization.get_vocabulary()\n",
    "\n",
    "# Creating index-word dictionary\n",
    "inverse_vocab = dict(enumerate(vocabulary))\n",
    "\n",
    "decoded_sentence = \" \".join(inverse_vocab[int(i)] for i in encoded_sentence)\n",
    "print(decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importantly, because TextVectorization is mostly a dictionary lookup operation, it can’t be executed on a GPU (or TPU)—only on a CPU.\n",
    "\n",
    "There are two ways we could use our TextVectorization layer. \n",
    "- The first option is to put it in the tf.data pipeline.\n",
    "- The second option is to make it part of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Pipeline\n",
    "\n",
    "# string_dataset would be a dataset that yields string tensors.\n",
    "int_sequence_dataset = string_dataset.map(\n",
    "    text_vectorization,\n",
    "    # The num_parallel_calls argument is used to parallelize the map() call across multiple CPU cores.\n",
    "    num_parallel_calls=4)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Part of Model\n",
    "\n",
    "# Create a symbolic input that expects strings\n",
    "text_input = keras.Input(shape=(), dtype=\"string\")\n",
    "\n",
    "# Apply the text vectorization layer to it\n",
    "vectorized_text = text_vectorization(text_input)\n",
    "\n",
    "# You can keep chaining new layers on top just your regular Functional API model.\n",
    "embedded_input = keras.layers.Embedding(...)(vectorized_text)\n",
    "output = ...\n",
    "\n",
    "model = keras.Model(text_input, output)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**\n",
    "- So if you’re training the model on GPU or TPU, you’ll probably want to go with the first option to get the best performance. \n",
    "\n",
    "- When training on a CPU, though, synchronous processing is fine: you will get 100% utilization of your cores regardless of which option you go with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare IMDB Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s start by downloading the dataset from the Stanford page of Andrew Maas and uncompressing it.\n",
    "\n",
    "`!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz`\n",
    "\n",
    "`!tar -xf aclImdb_v1.tar.gz`\n",
    "\n",
    "There’s also a train/unsup subdirectory in there, which we don’t need. Let’s delete it:\n",
    "\n",
    "`!rm -r aclImdb/train/unsup`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pathlib, shutil, random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining base directory\n",
    "base_dir = pathlib.Path(\"aclImdb\")\n",
    "\n",
    "# Val dir and Train dir\n",
    "val_dir = base_dir / \"val\"\n",
    "train_dir = base_dir / \"train\"\n",
    "\n",
    "# Category wise iterating\n",
    "for category in (\"neg\", \"pos\"):\n",
    "    # Creating folder\n",
    "    os.makedirs(val_dir / category)\n",
    "\n",
    "    # Getting all the file names from train directory\n",
    "    files = os.listdir(train_dir / category)\n",
    "\n",
    "    # Random shuffling of file names\n",
    "    random.Random(1337).shuffle(files)\n",
    "\n",
    "    # Number of samples\n",
    "    num_val_samples = int(0.2 * len(files))\n",
    "\n",
    "    # Getting the num_val_samples number of files from the end\n",
    "    val_files = files[-num_val_samples:]\n",
    "\n",
    "    # Moving the actual files from training folder to validation folder\n",
    "    for fname in val_files:\n",
    "        shutil.move(train_dir / category / fname,\n",
    "                    val_dir / category / fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batched Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 files belonging to 2 classes.\n",
      "Found 5000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/train\", batch_size=batch_size\n",
    ")\n",
    "val_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/val\", batch_size=batch_size\n",
    ")\n",
    "test_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/test\", batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape: (32,)\n",
      "inputs.dtype: <dtype: 'string'>\n",
      "targets.shape: (32,)\n",
      "targets.dtype: <dtype: 'int32'>\n",
      "inputs[0]: tf.Tensor(b\"I haven't yet read the Kurt Vonnegut book this was adapted from, but I am familiar with some of his other work and was interested to see how it would be translated to the screen. Overall, I think this is a very successful adaptation of one of Vonnegut's novels. It concerns the story of an American living in Germany who is recruited as a spy for the US. His job is to ingratiate himself with high ranked Nazi's and send secret messages to the American's via his weekly radio show. But when the war ends he is denounced as a war criminal but escapes to New York, where various odd plot twists await.<br /><br />If Mother Night has a problem it's that it tends to get a little too sentimental at times. But for most of the film the schmaltz is kept to a minimum and the very strange plot is carried through with skill and aplomb. And there are some fabulous moments of black comedy involving three right wing Christian fundamentalists and a very highly ranked Nazi in a prison cell. Very much recommended.\", shape=(), dtype=string)\n",
      "targets[0]: tf.Tensor(1, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Displaying the shapes and dtypes of the first training batch\n",
    "\n",
    "for inputs, targets in train_ds:\n",
    "    print(\"inputs.shape:\", inputs.shape)\n",
    "    print(\"inputs.dtype:\", inputs.dtype)\n",
    "\n",
    "    print(\"targets.shape:\", targets.shape)\n",
    "    print(\"targets.dtype:\", targets.dtype)\n",
    "    \n",
    "    print(\"inputs[0]:\", inputs[0])\n",
    "    print(\"targets[0]:\", targets[0])\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing words as a set: The bag-of-words approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SINGLE WORDS (UNIGRAMS) WITH BINARY ENCODING**\n",
    "\n",
    "The main advantage of this encoding is that you can represent an entire text as a single vector, where each entry is a presence indicator for a given word. \n",
    "\n",
    "For instance, using binary encoding (multi-hot), you’d encode a text as a vector with as many dimensions as there are words in your vocabulary—with 0s almost everywhere and some 1s for dimensions that encode words present in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TextVectorization object\n",
    "\n",
    "text_vectorization = TextVectorization(\n",
    "    max_tokens=20000,\n",
    "    output_mode=\"multi_hot\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a dataset  that only yields raw text inputs (no labels)\n",
    "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
    "\n",
    "# Use that dataset to index the dataset vocabulary via the adapt() method\n",
    "text_vectorization.adapt(text_only_train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "tf.Tensor(\n",
      "[b'So, what\\'s the reason? Is there some sort of vendetta against this AWESOME show or somebody involved therein? Why would the best show I\\'ve seen in years be canceled? I\\'m addicted. I saw this show on randomly last fall, and immediately loved it, and watched it every week. Then it went away, and I tried to Tivo it, but it wasn\\'t being aired. So I forgot about it for awhile, until I found the episodes on ABC\\'s website. Now I want MORE. I agree with everybody else - with the rest of the junk on TV today, it was refreshing to see something as well-rounded and developed as this. I watch Boston Legal for my eccentric-comedic fix, and House for my intellectual-mystery-jackass fix. My wife loves Grey\\'s Anatomy for its \"realism\", and I do love/hate the show, but it could not be farther from real for me. WAY too much drama. Everything that can go wrong, does. But for once, there\\'s a drama that\\'s REALLY real. Real people, real problems. Sure, there are some extremes like a former gangster turned good, girl running from the mob, etc., but these people (especially in NYC) are really out there, and I relate to each and every one of them. I can\\'t seem to get enough. I just hope that ABC will get their heads out of their bean-counting butts and continue this show. Get some respect for having a QUALITY drama out there. This could be one of the best shows of all time. If somebody will just let it.'\n",
      " b\"And I'll tell you why: whoever decided to edit this movie to make it suitable for television was very ill-advised. EVERYTHING CONCERNING DRUGS IS CUT OUT AND COVERED UP!!! How do they do it, you might ask? Well, they don't do it very well, that's for sure. Anyway, instead of the marijuana which Cheech and Chong are supposed to have in their possession, they are said to have diamonds! Still, the characters go around in a haze of marijuana smoke stoning others along their way with no explanation whatsoever!\"\n",
      " b'This movie was great the first time I saw it, when it was called \"Lost in Translation.\" But somehow Bill Murray turned into an eccentric black man played by Morgan Freeman, Scarlett Johansson turned into a cranky Latino woman played by Paz Vega, and Tokyo, Japan turned into Carson, California. Instead of meaningful conversations and silence we enjoyed in Translation, we get meaningless blabbering in 10 Items that verges on annoying. Instead of characters that were pensive and introspective as in Translation, we get characters that spew pointless advice on topics they have no clue about. How can a character that wears hundred dollar T-shirts and has never been inside a Target department store expect to give advice to a working-class woman on how to prepare for a job interview as an administrative assistant? Don\\'t think that stops him. If he isn\\'t giving her clothing advice, he\\'s telling her what she should eat. The most annoying part of the movie for me was how supposedly they were in a hurry to make an appointment, and yet the characters keep finding time to run another errand, be it washing the car, stopping at Arby\\'s, or just laying around to list off their 10 Items or Less lists of things they love and hate. I kept wanting to yell at them saying, \"Didn\\'t you say you had somewhere to be? What the heck are doing? A minute ago you were practically late, now you\\'re eating roast beef and pondering your lives!\" Until I saw this movie, I never truly understood how something could \"insist upon itself,\" but I think this movie does exactly that, and undeservedly so. The dialogue makes the characters cheesy and unsympathetic\\xc2\\x85with the exception that I felt sorry for both of the actors for having signed onto this project.'\n",
      " b\"The movie was excellent, save for some of the scenes with Esposito. I enjoyed how it brought together every detective on the series, and wrapped up some plotlines that were never resolved during the series (thanks to NBC...). It was great to see Pembleton and Bayliss together at their most human, and most basic persons. Braugher and Secor did a great job, but as usual will get overlooked. It hurt to see that this was the end of Homicide. Memories, tapes, and reruns on CourtTV just aren't the same as watching it come on every Friday. But the movie did its job and did it very well, presenting a great depiction of life after Al retired, and the family relationship that existed between the unit. I enjoyed this a lot.\"\n",
      " b'It\\'s so rare to find a film that provides a plot that can\\'t be figured out at every turn. Surprises throughout delighting throughout.<br /><br />A couple of things I could not quite understand though is that here you have the lead female character who plays \"the good girl, hard worker\" and yet in the end she ends up being an anti-social risk taker. That was not enough to make me not like it though but it did give me pause.<br /><br />The other thing I could not get was the old man who was being hauled away by the police at the very end of the film. I never did get who he was or what his issues were. If anyone has any ideas, I\\'d like to hear what they might be.<br /><br />The film was exciting and fun throughout and always left me guessing. Very much worth seeing.'\n",
      " b'Trash/bad movies usually ain\\'t bad because I will find them enjoyable. This one is so bad that I am out of words to describe it - its below \"bad\". There is an instruction in the beginning of the film that tell you what to do during the movie. Needless to say, the instruction and a dozen of beer couldn\\'t help me seat through the entire film. One tagliner compares this one to KILLBILL which is certainly unthinkable and an insult to our intelligent. Obviously. this tagliner had a plan to tempt you into buying this DVD.<br /><br />If you are considering renting this one, put it down! If you are thinking of buying, Dont think! If you unlucky to have this dvd, dont play it, throw it in trash bin immediately.'\n",
      " b'I really enjoyed this movie.I was fifteen when this movie came out and I could relate. This will be a movie I would show my kids to let them know, the feelings they are having are normal. It is funny to see how we could be so devestated by things at such a young age..who knew that we would bounce back....again and again....Great movie!!!!'\n",
      " b\"The film is based on Kipling's heroic lines that inspire Hollywood's biggest movie 1939.Out of the drumbeat rhythm of Kipling's most famous 85 lines rises a picture that will become known as the one great movie of the year.Big on the score of its armies in battle,its war elephants,its bandit hordes,its terror temples Thugs and mystic mountains of India .The picture is bigger still in its scope and sweep,is thrill and action but biggest biggest of all in the life breathes through three(Gary Grant,Victor McLagen and Douglas Fairbanks Jr) roaring,reckless,swaggering sons of the thundering gunfighters men who stride its mighty scenes in the flesh and blood of high adventure,it's a honest film of it all that makes Gunga Din a new experience in entertainment .Joan Fontaine gambled her against the valiant sergeants three.The romance between Fontaine and Fairbanks Jr aflame through dangerous days and nights of terror in a land where anything can happen. The motion picture has thrills for a thousand movies plundered for one mighty show.It's a fabulous,furious and far-flung adventure with the red-blood and gunpowder heroes who rise from the storied mystery of India and storm the screen with the lusty,rousing,robust life-thunder of men who fight for the love of it and love for the fun of it.The pictures is interpreted for the brave and roguish Gary Grant who rounded hundred villains Thugs and the mean Guru(Eduardo Ciannelli), Grant shouts : You're under arrest!.Besides is the heroic water man,Sam Jaffe,who regiment colonel(Montagu Love) says of him : You're a better man than I am,Gunga Din!\"\n",
      " b'Why me? Why should I be subjected to such slaughter of what could have made an interesting plot?! At least if I can warn other people off, it will have been worthwhile.<br /><br />I had to watch this horrible movie for a college course. Otherwise, I would have looked at the synopsis on the back of the thing and steered clear. The movie was slow, had PAINFULLY little character development, and centered around the idea that a creepy little white man can become cool if he hangs out with an LA-style token black man.<br /><br />If you want to experience the stereotypical LA feeling of dizzying superficiality - watch the movie. Note, though, that this movie does not DEPICT what we have come to think of as an \"LA lifestyle\", it is a wonderful example of the products that ARISE from it.'\n",
      " b\"DO NOT WATCH THIS SAD EXCUSE FOR A FILM. I have wasted time and money on this and am pretty p**sed off about it.<br /><br />The acting is comparable with high school plays. The script is shocking. There is no plot. Twenty minutes from the end (which I believe I should be rewarded for reaching) I had a headache from all the screaming, crying and wailing the five girls make.<br /><br />The majority of the violence is (rare for a film nowadays) suggested rather than graphically depicted but I found the characters so damn irritating that I wanted to see them, and indeed every single person involved in the making of this piece of s**t, die in the most horrible ways possible.<br /><br />I spend ten more minutes of my life saving you from a very poor 100 minutes of yours. Don't do it.\"\n",
      " b\"This Hitchcock movie bears little similarity to his later suspense films and seems much more like a very old fashioned morality tale. A young couple receives an inheritance that they believe will make them happy. They spend the money traveling about the world and living a very hedonistic existence. However, after a while the excitement begins to wane and the couple become dissipated and pointless in their existence. However, out of no where, when they are on a luxury cruise, the ship sinks and they lose everything--and end up much happier in the end because they now appreciate life! What an odd, silly and preachy film! Personally, I'd like to inherit all that money and find out if it makes me miserable!<br /><br />The production values are relatively poor compared to later productions--a rough film with poor sound quality and rather amateurish acting.\"\n",
      " b'The film began with Wheeler sneaking into the apartment of his girlfriend. Her aunt (Edna May Oliver--a person too talented for this film) didn\\'t like Wheeler--a sentiment I can easily relate to. The aunt decided to take this bland young lady abroad to get her away from Wheeler. They left and Wheeler invested in a revolution in a small mythical kingdom because they promised to make him their king. At about the same time, Woolsey was in the same small mythical kingdom and he was made king. So when Wheeler arrived, it was up to the boys to fight it out, but they refused because they are already friends--which greatly disappointed the people, as killing and replacing kings is a national pastime.<br /><br />I am a huge fan of comedy from the Golden Age of Hollywood--the silent era through the 1940s. I have seen and reviewed hundreds, if not thousands of these films and yet despite my love and appreciation for these films I have never been able to understand the appeal of Wheeler and Woolsey--the only comedy team that might be as bad as the Ritz Brothers! Despite being very successful in their short careers in Hollywood (cut short due to the early death of Robert Woolsey), I can\\'t help but notice that practically every other successful team did the same basic ideas but much better. For example, there were many elements of this film reminiscent of the Marx Brother\\'s film, DUCK SOUP, yet CRACKED NUTS never made me laugh and DUCK SOUP was a silly and highly enjoyable romp. At times, Woolsey talked a bit like Groucho, but his jokes never have punchlines that even remotely are funny! In fact, he just seemed to prattle pointlessly. His only funny quality was that he looked goofy--surely not enough reason to put him on film. Additionally, Wheeler had the comedic appeal of a piece of cheese--a piece of cheese that sang very poorly! A missed opportunity was the old Vaudeville routine later popularized by Abbott and Costello as \"who\\'s on first\" which was done in this film but it lacked any spark of wit or timing. In fact, soon after they started their spiel, they just ended the routine--so prematurely that you are left frustrated. I knew that \"who\\'s on first\" had been around for many years and used by many teams, but I really wanted to see Wheeler and Woolsey give it a fair shot and give it their own twist.<br /><br />Once again, I have found yet another sub-par film by this duo. While I must admit that I liked a few of their films mildly (such as SILLY BILLIES and THE RAINMAKERS--which I actually gave 6\\'s to on IMDb), this one was a major endurance test to complete--something that I find happens all too often when I view the films of Wheeler and Woolsey. Where was all the humor?!'\n",
      " b\"There's something wonderful about the fact that a movie made in 1934 can be head and shoulders above every Tarzan movie that followed it, including the bloated and boring 1980s piece Greystoke. Once the viewer gets past the first three scenes, which are admittedly dull, Tarzan and his Mate takes off like a shot, offering non-stop action, humor, and romance. Maureen O'Sullivan is charming and beautiful as Jane and walks off with the movie. Weismuller is solid as well. Highly recommended.\"\n",
      " b'\"October Sky\" is a film that will steal your heart, fill your mind with vivid imagery, and lift your spirit. The tale of Homer Hickham and his dream of creating a rocket seem so simple at first, especially when the film is set in a mining town, where the future is as clear cut as the lumps of coal in the mine. But Homer cannot follow in his father\\'s footsteps. With the encouragement of Miss Riley,(a friendly teacher), members of his father\\'s staff, and his friends, Homer attempts to make his dream a reality.<br /><br />Yet as in any true to life story, there are many stops along the way. Director Joe Johnston lowers us into the coal mines, where we witness the chilling plight of miners stooped beneath a ceiling of rock. With lit helmets and bent posture, they resembled alien insectoids more than humans in the darkness. The hacking coughs of the miners and the blackened faces were a constant reminder of the danger the miners faced in their work.<br /><br />Contrasting the mine shaft\\'s lugubrious load are the images of Homer and his friend\\'s rocket launches. Underneath the blue bowl of sky, rockets are placed upon a pad and launched into the stratosphere...And nothing can match the scene when Homer sees Sputnik for the first time.<br /><br />Yet what makes the film so endearing is the relationship between the characters. Homer\\'s father is a classic hardened man...but he has a soft side as well. We see that he does love his son, despite their many arguments. The love and support of Miss Riley is evident as well. Best of all, the film is uncomfortable. It doesn\\'t tie everything up in a nice bow. It tears at you, lifts you up. It keeps an air of reality, which is important in a film like this.<br /><br />This film can be considered a complete work. At first, I was disappointed that the film did not continue with Homer\\'s life. I didn\\'t want it to end. Then I realized...that\\'s what a good film does to a person. If it has done its job, you won\\'t want it to end. And \"October Sky\" accomplishes just that.'\n",
      " b'Despite the other comments listed here, this is probably the best Dirty Harry movie made; a film that reflects -- for better or worse -- the country\\'s socio-political feelings during the Reagan glory years of the early \\'80\\'s. It\\'s also a kickass action movie.<br /><br />Opening with a liberal, female judge overturning a murder case due to lack of tangible evidence and then going straight into the coffee shop encounter with several unfortunate hoodlums (the scene which prompts the famous, \"Go ahead, make my day\" line), \"Sudden Impact\" is one non-stop roller coaster of an action film. The first time you get to catch your breath is when the troublesome Inspector Callahan is sent away to a nearby city to investigate the background of a murdered hood. It gets only better from there with an over-the-top group of grotesque thugs for Callahan to deal with along with a sherriff with a mysterious past. Superb direction and photography and a at-times hilarious script help make this film one of the best of the \\'80\\'s.'\n",
      " b\"I first saw this back in the early 90s on UK TV, i did like it then but i missed the chance to tape it, many years passed but the film always stuck with me and i lost hope of seeing it TV again, the main thing that stuck with me was the end, the hole castle part really touched me, its easy to watch, has a great story, great music, the list goes on and on, its OK me saying how good it is but everyone will take there own best bits away with them once they have seen it, yes the animation is top notch and beautiful to watch, it does show its age in a very few parts but that has now become part of it beauty, i am so glad it has came out on DVD as it is one of my top 10 films of all time. Buy it or rent it just see it, best viewing is at night alone with drink and food in reach so you don't have to stop the film.<br /><br />Enjoy\"\n",
      " b\"One of the best Tarzan films is also one of its most action packed (and graphic).<br /><br />Picking up a year or so after Tarzan the Ape Man, Niel Hamilton's Holt has asked a rich friend to finance a safari back to the elephants graveyard to collect ivory. His Friend arrives also carrying dresses and perfumes that Holt hopes to use to win Jane back from Tarzan. Before they can leave Holt finds his map stolen and it becomes a mad dash to try and capture a competing expedition. When they finally over take the thieves they find the whole party dead and themselves surrounded. They have no choice but to fight their way out and soon find they are out of the frying pan and into the fire. Eventually Tarzan and Jane show up and everyone is off on even more adventures.<br /><br />Infamous film was heavily censored to reduce the graphic violence (Its graphic even by todays standards. It probably would get a PG 13) and to remove all hint of nudity, (there is a several minute long nude swim scene involving Jane that is full frontal in its nudity, it was only recently restored). Its clear watching the restored version why this film was reduced by 20 minutes in its run time for TV. As it stands in its restored version this is a very adult film that is romantic, touching, action filled and everything else that a movie should be. Its an amazing film by almost any standard. Best of all its the sort of film that plays well both as a stand alone adventure, one need not to have seen the first film to enjoy it, but its also a film that deepens the characters and themes that were set up in that original film. Its an amazing thing.<br /><br />I really like this film a great deal.<br /><br />If there are any flaws to the film, its perhaps that the film hasn't aged well. The rear screen is often very obvious, there are gorilla suits for many of the apes and some of the other effects are more quaint rather than convincing. However on almost every other level this film is top notch.<br /><br />You really owe it to yourself to see this. Make yourself a big bowl of popcorn and curl up on the couch and just let yourself drift back to a simpler time. This is one of the great adventures.\"\n",
      " b'This movie is one of those \"WOW!\" movies. Not because it\\'s the greatest movie of all time, but because it surprised me. Not only was it a T.V. movie, but it was on Elvis. I can safely say as many impersonators as there are there was only one Elvis, but I can also safely say that Kurt Russel came extremely close to being the real thing. It was one of the greatest impersonations that I have ever seen. He had me believing that it was really him. I learned a lot about Elvis\\' life from watching this movie. And don\\'t led the television part of it let you stray-it\\'s actually a really fantastic film! And Kurt Russel could\\'ve been Elvis\\' twin :)'\n",
      " b'Ridiculous-looking little boogers that spawn foam and reproduce themselves. So far for the horror-elements this movie has. All the rest of MUNCHIES plays out like a really retarded comedy that\\'s so stupid you won\\'t find it funny anymore after about 15 minutes. I can imagine little kids cheering for these little boogers, but adults will be left with only those supposedly \"smart\" references translating to on-screen stuff like Capt. Kirk\\'s log entries from STAR TREK, the most well-known scene from E.T., a blatant statement from the filmmakers going \"Look! We\\'re cashing in on GREMLINS\\' success here!\" and a cardboard cut-out of Clint Eastwood telling us... what about his western movies exactly? That last one was totally lost on me... Oh yes, and chemical waste disposal in caves seems to be a bad thing. Don\\'t know where they got that idea from.<br /><br />Not to say that MUNCHIES is the most insufferable film to sit through, for that matter. It\\'s just really, really dumb. And if you manage to crack a smile while watching it, you\\'ll probably feel as dumb yourself for having done that after the film\\'s finished.<br /><br />Good Badness? Yes, but only if \"dumb\", \"retarded\" & \"ridiculous\" are criteria you\\'re looking for. 3/10 and, well, uhm, 6/10.'\n",
      " b'Nina Foch insists that \"My Name is Julia Ross\" in this 1945 film noir also starring Dame May Witty and George Macready. It\\'s short, and because it is, the film suffers. It could have stood to have been a good fifteen minutes to a half hour longer.<br /><br />When I was growing up, Foch was a fixture on television, playing a neurotic woman, the wife with the cheating husband, the nervous wreck. She became one of the great acting teachers in Los Angeles. Here, she\\'s a pretty young ingenue playing the title role. Julia answers an ad for a secretary and is hired immediately by Mrs. Hughes and her son Ralph. Little does she know - though we learn immediately - that the employment agent is a front, set up to get just the right woman for this assignment, a woman with no family and no boyfriend.<br /><br />It\\'s a live-in situation; once Julia gets to the house, she\\'s drugged, and when she wakes up, she\\'s told she\\'s Mrs. Hughes and not allowed to leave.<br /><br />The acting is very good. Low budget but still entertaining - some things, particularly at the end, happen way too quickly, which is why I said the movie is too short. Nevertheless, I recommend it.'\n",
      " b\"This movie is so, so, so horrible, that it makes angels lose their wings. Shaq had tried to make other crossover efforts, like his work in Shaq-Fu for the NES and his plethora of unbearable rap albums, and later, the epic serving of horrible film-making that is Steel.<br /><br />There's not a single good thing to be said about this movie. I saw it a bunch of times when I was very young, but I must've been an idiot then, because this movie takes all that is enjoyable about films and tears it apart. It's fun to mock. I saw it on the Disney Channel a while back and spent a few minutes doing that. Although, once the thrill of mocking it is done, you still become overwhelmed by its terribleness.<br /><br />If you see it on TV, try this: consider, as your watching the film, removing from it all the scenes in which Shaq uses his magical genie powers. If you do that, it becomes like a film about a pedophile chasing a kid and rapping to seduce him. That's kinda funny, and disturbing.<br /><br />A horrible example of film. Do not, unless looking to mock it, see this movie.\"\n",
      " b'Worst movie of all time? Wow, whoa now. You cannot be serious.<br /><br />Maybe it\\'s all about what you expect a movie to do to you. I live in Oregon, so I got to enjoy the beautifully-filmed shots of familiar yet still amazingly beautiful Smith Rocks and other areas in Central Oregon (as well as the sweet cameo of our own Ken Kesey and Ken Babbs looking down on baby Sissy\\'s cradle at the beginning of the movie). Those alone were enough to spur me to give the movie a better than \"average\" score.<br /><br />Or .... Maybe it\\'s all about what expectations you have. Having read the book AGES ago, and thinking to myself \"goodness, no one could ever make a movie out of this interesting, quirky, weird book ... especially 20 years later, when mores (MORAYS -- can\\'t put in the accent mark online) have changed\" -- I was actually quite pleasantly surprised when I first watched the movie when it came out in 1994 and even liked it more today watching it again.<br /><br />Sissy was exquisitely cast, and I don\\'t care what you all say, I was also pleasantly surprised at Rain Phoenix\\'s and John Hurt\\'s performances. I am not a lesbian nor bi nor trans, but have met many folks who are similar to the folks they were supposed to portray -- and those \"real\" folks kinda acted the same way as these actors acted. Stilted a bit, stage-ey -- always a bit \"on.\" Gus Van Sant is one weird native Oregonian but by garsh he done a good job adapting this crazy book, IMHO.'\n",
      " b'This is a film that the mainstream market will probably never be able to access as it doesn\\'t exactly give the viewer easy watching. The story about troubled Spike and his friend Heaton is not exactly a Friday night film yet it has its own unique edge and I found that it was entertaining. There are moments of brilliance given that the film was shot on such a low budget, such as when Spike inhales the aerosol. However I did not really understand the relationship between Spike and Heaton and to be honest it made me spend most of the film trying to work it out. And also I did not like the fact that most of the film is spent with the two friends talking and not really much \"action\". It is a small film that is complex to watch and that is what makes it appealing.'\n",
      " b\"This is an evocative and idealized portrait of the early life of Lincoln(born 1809 Hodgensville-Kentucky- and died in Washington 1865). Ford's excellent movie takes Abraham Lincoln(Fonda) from his youth. He studied laws, common law and began practice as lawyer in 1837.This Hollywood biography follows Lincoln from his log-cabin days, initial relationship to Mary Todd(Weaver), following the couple from their first ball,and his departure for congress candidate. But focuses mainly on a brothers(Richard Cromwell,Eddie Quillan) accused for murder, the posterior trial with amusing court debate scenes and the protection for their mum(Alice Brady). The Lincoln-Fonda as defender advocate and Donald Meek-prosecutor are nothing short of brilliant.<br /><br />Excellent performance from Henry Fonda as idealistic,traveller Springfield solicitor , he was to star regularly for Ford from this movie, as \\xc2\\xa8Grapes of wrath,My darling Clementine, and Fort Apache\\xc2\\xa8. Besides sterling acting by Alice Brady as grieved mother, she was a great actress from the silent cinema, but this one results to be her last movie because she early died from cancer.The Lincoln's deeds developing make for skilfully appealing entertaining.His portrayal shows a nostalgic longing for things past and old values and describes his goodness,uprightness and willful. Lincoln, like John Ford, was a straightforward man who never varied the ideals of his youth.This American masterpiece is correct on both counts, as splendid biography and as magnificent drama.<br /><br />Another biographies about Abraham Lincoln are the following: 1) \\xc2\\xa8Abraham Lincoln\\xc2\\xa8(1930) by D.W.Griffith with Walter Huston, Una Merkel, talking from his birth until his assassination; 2) \\xc2\\xa8Abe Lincoln in Illinois\\xc2\\xa8(1940)by John Cromwell with Raymond Massey, Ruth Gordon, concerning similar events to Ford's film through his career as lawyer 3)TV version titled \\xc2\\xa8Gore Vidal's Lincoln\\xc2\\xa8 with Sam Waterston and Mary Tyler Moore as Mary Todd.\"\n",
      " b\"I saw this when I was 17 and haven't seen it since. The 'CBS Late Movie' used to show it on a regular basis at one point. I remember how sad and upsetting it was, it truly made me sick to my stomach. Effects then weren't what they are today, but nevertheless, it conveyed the feeling of being alone in the Amazon, after losing both parents and searching for a way out, very well. I remember the bugs and maggots the most, so realistic they were, eating her flesh. It's a dark film which was controversial subject matter at the time, even though likely it was strongly edited for TV. I wish I remembered more details, and if I ever get the chance to see it again, I can comment more. I have been looking for this for years. I believe it may have been shown on CBS under yet another title. I have no idea whether it was ever released on video.\"\n",
      " b\"it's a very nice movie and i would definitely recommend it to everyone. but there are 2 minus points: - the level of the stories has a large spectrum. some of the scenes are very great and some are just boring. - a lot of stories are not self-contained (if you compare to f.e. coffee and cigarettes, where each story has a point, a message, a punchline or however you wanna call it) but well, most stories are really good, some are great and overall it's one of the best movies this year for sure!<br /><br />annoying, that i have to fill 10 lines at minimum, i haven't got more to say and i don't want to start analyzing the single sequences...<br /><br />well, i think that's it!\"\n",
      " b\"Unless you are an Evangelical Christian then make like an Egyptian and avoid like the biblical plague.<br /><br />Awful - why oh why does IMDb list the most favourable reviews at the top of the list - it was due to one of these that I have just wasted the end of what started out as good evening on this claptrap.<br /><br />The plot premise started out strong enough - I was drawn into the film and was interested right up to the point where the Bible sermons took over. What a waste.<br /><br />This film has so incensed me that I have registered with IMDb for the first time just to complain about it - I hope at least that by doing so I save someone else's evening.<br /><br />Hay - what a Christian act on my part ;-)\"\n",
      " b'Another comment about this film made it sound lousy. Given talking pictures were so new - I think the script and acting were good. Davis was so young and fresh. She had not yet found her own style that we had grown to expect. Yet it is great to see her this way - still learning the craft.<br /><br />So many clich\\xc3\\xa9s came from this film and it seems, this film blazed some trails for the next 70 years. My vote is see it and remember how young this type of film was. Keep and open mind and you maybe shocked at how troubled the characters were in this picture, for being 1934 and how we view the early part of last century as uptight.. I love it and hope you make up your own mind about it not influenced by others negative and one note comments.'\n",
      " b'This movie really deserves the MST3K treatment. A pseudo-ancient fantasy hack-n-slash tale featuring twin barbarian brothers with a collective IQ of hot water, character names that seem to have been derived from a Mad Libs book, and such classic lines as \"Hold her down and uncover her belly!\", The Barbarians crosses over into the \"so bad, it\\'s good\" territory.'\n",
      " b'This review also contains a spoiler of the first movie -- so if you haven\\'t seen either movie and want to but don\\'t want the spoilers, please don\\'t read this review!<br /><br />While this movie is supposed to be about Christian and Kathryn meeting for the first time, the movie is a poor copy of the first Cruel Intentions. The actors that they had portray Ryan Phillippe\\'s Christian and Sarah Michelle Gellar\\'s Kathryn are very poor substitutes indeed. Neither can pull off the smarmy, snooty rich-kid attitude that the original actors did. It\\'s absolutely appalling that some of the dialog was verbatim -- not so much between Christian and Kathryn, but if you listen closely enough you\\'ll recognize it. There are also inconsistencies in the plot - if this were truly the first meeting of Christian and Kathryn, then why is it that Christian fell in love with a girl at the end of the movie? He supposedly was supposed to be in love for the first time in the original movie (with Reese Witherspoon\\'s character).<br /><br />Also, the tie-in with the photography/\"You could be a model\" comment at the end was totally lame and didn\\'t add anything at all. Overall, this movie was a waste of time. I can\\'t believe they made a Cruel Intentions 3.'\n",
      " b\"Douglas Sirk directs this over-acted drama about the unhappy affluent. Kyle Hadley(Robert Stack)and Mitch Wayne(Rock Hudson) are boyhood friends with different looks on life. Kyle is the womanizing son of an oil tycoon; Mitch works for the Hadley Oil Company. Both fall in love with the same woman, Lucy Moore; but it is Kyle that has the means to wow her off her feet and marry her. Sister Marylee(Dorothy Malone)seems to be the town's nymphomaniac and carrying a torch for Mitch, who always seems to be the one to clean up the Hadley's messes. Ambitious with pretension; a little over the top, but the stars make it a movie to see. I was most impressed with Malone. Rounding out the cast: Robert Keith, Edward Platt, John Lurch and Robert J. Wilke.\"\n",
      " b'No, not really, but this is a very good film indeed, and is sadly a forgotten gem. Black and white suits the film.<br /><br />Straight forward formula, a guy had the plague and the authorities have to track down everyone he came in contact with before they die.<br /><br />Very well directed, and the acting is great. Richard Widmark as the male lead is good but is completely over shadowed in the acting stakes by Paul Douglas as the police captain, and Jack Palance (never better than this) and Zero Mostel as the baddies. Sadly Palance went on to play similar characters in some really second rate gangster or war movies.'], shape=(32,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# Looking at text_only_train_ds first batch\n",
    "\n",
    "for i in text_only_train_ds:\n",
    "    print(len(i))\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625\n",
      "20000\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "for i in text_only_train_ds:\n",
    "    count += 1\n",
    "\n",
    "# Total number of batches in the training set\n",
    "print(count)\n",
    "\n",
    "# Crosschecking the total count of samples in the training dataset\n",
    "print(count*32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[UNK]', 'the', 'and', 'a', 'of', 'to', 'is', 'in', 'it', 'i', 'this', 'that', 'br', 'was', 'as', 'for', 'with', 'movie', 'but', 'film']\n"
     ]
    }
   ],
   "source": [
    "# First 20 words in the vocab list\n",
    "print(text_vectorization.get_vocabulary()[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(20,), dtype=float32, numpy=\n",
       "array([1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample text vectorization\n",
    "\n",
    "sample_text = \"The is asdasd in.\"\n",
    "\n",
    "text_vectorization(sample_text)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare processed versions of our training, validation, and test dataset\n",
    "# Make sure to specify num_parallel_calls to leverage multiple CPU cores\n",
    "\n",
    "binary_1gram_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "\n",
    "binary_1gram_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "\n",
    "binary_1gram_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "(<tf.Tensor: shape=(32, 20000), dtype=float32, numpy=\n",
      "array([[1., 1., 1., ..., 0., 0., 0.],\n",
      "       [1., 1., 1., ..., 0., 0., 0.],\n",
      "       [1., 1., 1., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [1., 1., 1., ..., 0., 0., 0.],\n",
      "       [1., 1., 1., ..., 0., 0., 0.],\n",
      "       [1., 1., 1., ..., 0., 0., 0.]], dtype=float32)>, <tf.Tensor: shape=(32,), dtype=int32, numpy=\n",
      "array([1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,\n",
      "       0, 1, 0, 1, 0, 0, 1, 0, 0, 0])>)\n"
     ]
    }
   ],
   "source": [
    "# binary_1gram_train_ds - This is a pack of 2 tensor set\n",
    "# 1st set - input texts\n",
    "# 2nd set - corresponding labels\n",
    "for i in binary_1gram_train_ds:\n",
    "    print(len(i))\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "tf.Tensor(\n",
      "[[1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]], shape=(32, 20000), dtype=float32)\n",
      "32\n",
      "tf.Tensor([0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 1 1 0], shape=(32,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Input set - 32 - Each having length 20000 (Vocab size)\n",
    "# Label set - 32 - Labels for 32 corresponding input \n",
    "for i in binary_1gram_train_ds:\n",
    "    for j in i:\n",
    "        print(len(j))\n",
    "        print(j)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape: (32, 20000)\n",
      "inputs.dtype: <dtype: 'float32'>\n",
      "targets.shape: (32,)\n",
      "targets.dtype: <dtype: 'int32'>\n",
      "inputs[0]: tf.Tensor([1. 1. 1. ... 0. 0. 0.], shape=(20000,), dtype=float32)\n",
      "targets[0]: tf.Tensor(1, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Inspecting a single batch\n",
    "\n",
    "for inputs, targets in binary_1gram_train_ds:\n",
    "    print(\"inputs.shape:\", inputs.shape)\n",
    "    print(\"inputs.dtype:\", inputs.dtype)\n",
    "\n",
    "    print(\"targets.shape:\", targets.shape)\n",
    "    print(\"targets.dtype:\", targets.dtype)\n",
    "\n",
    "    print(\"inputs[0]:\", inputs[0])\n",
    "    print(\"targets[0]:\", targets[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**\n",
    "\n",
    "This vectorization only encodes the information about the words that are present in the text. But it does not encode the sequence (position of each word in the sentence)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-building utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(max_tokens=20000, hidden_dim=16):\n",
    "    # Flattened vector representation of a text (vector of shape max_tokens)\n",
    "    inputs = keras.Input(shape=(max_tokens,))\n",
    "\n",
    "    # First hidden layer\n",
    "    x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
    "    # Second hidden layer\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    \n",
    "    # Output layer\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    \n",
    "    # Building and compiling the model\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer=\"rmsprop\",\n",
    "                  loss=\"binary_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 20000)]           0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 16)                320016    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 17        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320033 (1.22 MB)\n",
      "Trainable params: 320033 (1.22 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 18s 27ms/step - loss: 0.4203 - accuracy: 0.8227 - val_loss: 0.3063 - val_accuracy: 0.8786\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.2889 - accuracy: 0.8966 - val_loss: 0.3024 - val_accuracy: 0.8846\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.2575 - accuracy: 0.9104 - val_loss: 0.3171 - val_accuracy: 0.8846\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 5s 7ms/step - loss: 0.2434 - accuracy: 0.9149 - val_loss: 0.3353 - val_accuracy: 0.8806\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.2311 - accuracy: 0.9221 - val_loss: 0.3553 - val_accuracy: 0.8776\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.2283 - accuracy: 0.9228 - val_loss: 0.3826 - val_accuracy: 0.8720\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.2231 - accuracy: 0.9232 - val_loss: 0.3907 - val_accuracy: 0.8684\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.2157 - accuracy: 0.9278 - val_loss: 0.4015 - val_accuracy: 0.8712\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 5s 9ms/step - loss: 0.2094 - accuracy: 0.9304 - val_loss: 0.4247 - val_accuracy: 0.8662\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.2092 - accuracy: 0.9309 - val_loss: 0.4393 - val_accuracy: 0.8606\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1f8bc3be4a0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training and testing the binary unigram model\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"binary_1gram.keras\",\n",
    "                                    save_best_only=True)\n",
    "                                    ]\n",
    "\n",
    "model.fit(binary_1gram_train_ds.cache(),\n",
    "          validation_data=binary_1gram_val_ds.cache(),\n",
    "          epochs=10,\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 15s 19ms/step - loss: 0.2915 - accuracy: 0.8861\n",
      "Test acc: 0.886\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model(\"binary_1gram.keras\")\n",
    "\n",
    "print(f\"Test acc: {model.evaluate(binary_1gram_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in this case, since the data set is a balanced two-class classification dataset (there are as many positive samples as\n",
    "negative samples), the “naive baseline” we could reach without training an actual model would only be 50%. \n",
    "\n",
    "Meanwhile, the best score that can be achieved on this dataset without leveraging external data is around 95% test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BIGRAMS WITH BINARY ENCODING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(\n",
    "    ngrams=2,\n",
    "    max_tokens=20000,\n",
    "    output_mode=\"multi_hot\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "binary_2gram_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "\n",
    "binary_2gram_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "\n",
    "binary_2gram_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 20000)]           0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 16)                320016    \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320033 (1.22 MB)\n",
      "Trainable params: 320033 (1.22 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 35s 55ms/step - loss: 0.4003 - accuracy: 0.8344 - val_loss: 0.2866 - val_accuracy: 0.8896\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.2537 - accuracy: 0.9097 - val_loss: 0.2943 - val_accuracy: 0.8876\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.2064 - accuracy: 0.9319 - val_loss: 0.3126 - val_accuracy: 0.8912\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.1883 - accuracy: 0.9422 - val_loss: 0.3321 - val_accuracy: 0.8872\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.1692 - accuracy: 0.9484 - val_loss: 0.3535 - val_accuracy: 0.8882\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.1721 - accuracy: 0.9516 - val_loss: 0.3691 - val_accuracy: 0.8908\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.1615 - accuracy: 0.9528 - val_loss: 0.3941 - val_accuracy: 0.8878\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.1560 - accuracy: 0.9557 - val_loss: 0.4041 - val_accuracy: 0.8886\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.1550 - accuracy: 0.9589 - val_loss: 0.4176 - val_accuracy: 0.8852\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.1498 - accuracy: 0.9588 - val_loss: 0.4329 - val_accuracy: 0.8856\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1f8c86ec100>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"binary_2gram.keras\",\n",
    "    save_best_only=True)\n",
    "    ]\n",
    "\n",
    "model.fit(binary_2gram_train_ds.cache(),\n",
    "          validation_data=binary_2gram_val_ds.cache(),\n",
    "          epochs=10,\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 109s 138ms/step - loss: 0.2761 - accuracy: 0.8948\n",
      "Test acc: 0.895\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model(\"binary_2gram.keras\")\n",
    "print(f\"Test acc: {model.evaluate(binary_2gram_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance increased from 88.6% to 89.5%. It's a good increase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BIGRAMS WITH TF-IDF ENCODING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also add a bit more information to this representation by counting how many times each word or N-gram occurs, that is to say, by taking the histogram of the words over the text.\n",
    "\n",
    "If you’re doing text classification, knowing how many times a word occurs in a sample is critical: any sufficiently long movie review may contain the word “terrible” regardless of sentiment, but a review that contains many instances of the word “terrible” is\n",
    "likely a negative one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(\n",
    "    ngrams=2,\n",
    "    max_tokens=20000,\n",
    "    output_mode=\"count\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, of course, some words are bound to occur more often than others no matter what the text is about. The words “the,” “a,” “is,” and “are” will always dominate your word count histograms, drowning out other words—despite being pretty much useless features in a classification context. How could we address this?\n",
    "\n",
    "We could just normalize word counts by subtracting the mean and dividing by the variance (computed across the entire training  dataset). That would make sense. Except most vectorized sentences consist almost entirely of zeros (our previous example features 12 non-zero entries and 19,988 zero entries), a property called “sparsity.” That’s a great property to have, as it dramatically\n",
    "reduces compute load and reduces the risk of overfitting. If we subtracted the mean from each feature, we’d wreck sparsity. Thus, whatever normalization scheme we use should be divide-only.\n",
    "\n",
    "The best practice is to go with something called TF-IDF normalization.\n",
    "\n",
    "```\n",
    "def tfidf(term, document, dataset):\n",
    "    term_freq = document.count(term)\n",
    "    doc_freq = math.log(sum(doc.count(term) for doc in dataset) + 1)\n",
    "    return term_freq / doc_freq\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(\n",
    "    ngrams=2,\n",
    "    max_tokens=20000,\n",
    "    output_mode=\"tf_idf\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "tfidf_2gram_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "\n",
    "tfidf_2gram_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "\n",
    "tfidf_2gram_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 20000)]           0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 16)                320016    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320033 (1.22 MB)\n",
      "Trainable params: 320033 (1.22 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 19s 28ms/step - loss: 0.5309 - accuracy: 0.7721 - val_loss: 0.2973 - val_accuracy: 0.8848\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 5s 7ms/step - loss: 0.3408 - accuracy: 0.8616 - val_loss: 0.3126 - val_accuracy: 0.8862\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.3105 - accuracy: 0.8742 - val_loss: 0.3279 - val_accuracy: 0.8644\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.2766 - accuracy: 0.8885 - val_loss: 0.3326 - val_accuracy: 0.8654\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.2644 - accuracy: 0.8958 - val_loss: 0.3422 - val_accuracy: 0.8732\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.2480 - accuracy: 0.9044 - val_loss: 0.3378 - val_accuracy: 0.8716\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.2339 - accuracy: 0.9076 - val_loss: 0.3620 - val_accuracy: 0.8514\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.2246 - accuracy: 0.9112 - val_loss: 0.3630 - val_accuracy: 0.8492\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.2178 - accuracy: 0.9133 - val_loss: 0.3911 - val_accuracy: 0.8450\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.2101 - accuracy: 0.9193 - val_loss: 0.3737 - val_accuracy: 0.8548\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1f86745d3c0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training and testing the TF-IDF bigram model\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"tfidf_2gram.keras\",\n",
    "                                    save_best_only=True)\n",
    "                                    ]\n",
    "\n",
    "model.fit(tfidf_2gram_train_ds.cache(),\n",
    "          validation_data=tfidf_2gram_val_ds.cache(),\n",
    "          epochs=10,\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 15s 19ms/step - loss: 0.2989 - accuracy: 0.8817\n",
      "Test acc: 0.882\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model(\"tfidf_2gram.keras\")\n",
    "print(f\"Test acc: {model.evaluate(tfidf_2gram_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance decreased from 89% to 88.2%. Not really helpful in this case. However, for many text-classification datasets, it\n",
    "would be typical to see a one-percentage-point increase when using TF-IDF compared to plain binary encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the preceding examples, we did our text standardization, splitting, and indexing as part of the tf.data pipeline. \n",
    "\n",
    "But if we want to export a standalone model independent of this pipeline, we should make sure that it incorporates its own text preprocessing (otherwise, you’d have to reimplement in the production environment, which can be challenging or can lead to subtle discrepancies between the training data and the production data).\n",
    "\n",
    "Just create a new model that reuses your TextVectorization layer and adds to it the model you just trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(\n",
    "    ngrams=2,\n",
    "    max_tokens=20000,\n",
    "    output_mode=\"multi_hot\",\n",
    ")\n",
    "\n",
    "text_vectorization.adapt(text_only_train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"binary_2gram.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One input sample would be one string\n",
    "inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
    "\n",
    "# Apply text preprocessing\n",
    "processed_inputs = text_vectorization(inputs)\n",
    "\n",
    "# Apply the previously trained model\n",
    "outputs = model(processed_inputs)\n",
    "\n",
    "# Instantiate the end-to-end mode\n",
    "inference_model = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82.35 percent positive\n"
     ]
    }
   ],
   "source": [
    "raw_text_data = tf.convert_to_tensor([\n",
    "    [\"That was an excellent movie, I loved it.\"],\n",
    "    ])\n",
    "\n",
    "predictions = inference_model(raw_text_data)\n",
    "print(f\"{float(predictions[0] * 100):.2f} percent positive\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
