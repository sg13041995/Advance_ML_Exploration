{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Dense Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense Class\n",
    "class NaiveDense:\n",
    "    # constructor method\n",
    "    def __init__(self, input_size, output_size, activation):\n",
    "        # We can use actiavtion function of choice like ReLU, Softmax etc.\n",
    "        self.activation = activation\n",
    "        \n",
    "        # Defining weight matrix shape\n",
    "        w_shape = (input_size, output_size)\n",
    "\n",
    "        # Creating weight matrix with random values\n",
    "        w_initial_value = tf.random.uniform(w_shape, minval=0, maxval=1e-1)\n",
    "\n",
    "        # Initializing tensorflow variable with randomly initialized weight values\n",
    "        self.W = tf.Variable(w_initial_value)\n",
    "\n",
    "        # defining output shape\n",
    "        b_shape = (output_size,)\n",
    "\n",
    "        # Creating output matrix with zeros\n",
    "        b_initial_value = tf.zeros(b_shape)\n",
    "\n",
    "        # Initializing tensorflow variable with output matrix values\n",
    "        self.b = tf.Variable(b_initial_value)\n",
    "    \n",
    "    # Tensorflow specific method to apply the forward pass\n",
    "    def __call__(self, inputs):\n",
    "        # Feeding the dot product of input and weight (with added bias) into actiavtion function of choice\n",
    "        # Returning the output value\n",
    "        return self.activation(tf.matmul(inputs, self.W) + self.b)\n",
    "    \n",
    "\n",
    "    # Method to access the layer parameters (weights and bias) as property\n",
    "    @property\n",
    "    def weights(self):\n",
    "        return [self.W, self.b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Sequential Class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential Class\n",
    "class NaiveSequential:\n",
    "    # constructor method\n",
    "    def __init__(self, layers):\n",
    "        # a sequence of dense layers\n",
    "        self.layers = layers\n",
    "    \n",
    "    # calls the underlying layers on the inputs, in order\n",
    "    def __call__(self, inputs):\n",
    "        x = inputs\n",
    "        for layer in self.layers:\n",
    "            # output of the first layer will be the input of the second layer\n",
    "            x = layer(x)\n",
    "        # Returning the output of the final layer\n",
    "        return x\n",
    "    \n",
    "    # Method to access the layer parameters (weights and bias) as property\n",
    "    @property\n",
    "    def weights(self):\n",
    "        # List to store the weights of all the lareys\n",
    "        weights = []\n",
    "        # Accessing and storing the weights of individual layers into the list\n",
    "        for layer in self.layers:\n",
    "            weights += layer.weights\n",
    "        # Returning the complete list of weights of all the layers\n",
    "        return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mock Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying - Input, Output size and ReLU as activation\n",
    "# We have two Dense layers\n",
    "model = NaiveSequential([\n",
    "    NaiveDense(input_size=28 * 28, output_size=512, activation=tf.nn.relu),\n",
    "    \n",
    "    # Output layer with softmax activation\n",
    "    NaiveDense(input_size=512, output_size=10, activation=tf.nn.softmax)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 512)\n",
      "(512,)\n",
      "(512, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "# First hidden layer input shape\n",
    "print(model.weights[0].shape)\n",
    "# First hidden layer output shape\n",
    "print(model.weights[1].shape)\n",
    "\n",
    "# Second hidden layer input shape\n",
    "print(model.weights[2].shape)\n",
    "# Second hidden layer output shape\n",
    "print(model.weights[3].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Batch Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Generator Class\n",
    "class BatchGenerator:\n",
    "    # constructor method\n",
    "    def __init__(self, images, labels, batch_size=128):\n",
    "        # Staring index 0\n",
    "        self.index = 0\n",
    "\n",
    "        # Images\n",
    "        self.images = images\n",
    "\n",
    "        # Labels\n",
    "        self.labels = labels\n",
    "\n",
    "        # Batch size\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Calculating the number of batches\n",
    "        self.num_batches = math.ceil(len(images) / batch_size)\n",
    "    \n",
    "\n",
    "    def next(self):\n",
    "        # Selecting features/images based on current index and batch size\n",
    "        images = self.images[self.index : self.index + self.batch_size]\n",
    "        \n",
    "        # Selecting labels based on current index and batch size\n",
    "        labels = self.labels[self.index : self.index + self.batch_size]\n",
    "\n",
    "        # Incrementing current index by batch size\n",
    "        self.index += self.batch_size\n",
    "\n",
    "        # Returning current set of features/images and labels\n",
    "        return images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One Training Step Calculating Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Process:** Updating the weights of the model after running it on one batch of data\n",
    "\n",
    "**Steps:**\n",
    "1. Compute the predictions of the model for the images in the batch.\n",
    "2. Compute the loss value for these predictions, given the actual labels\n",
    "3. Compute the gradient of the loss with regard to the model’s weights.\n",
    "4. Move the weights by a small amount in the direction opposite to the gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tensorflow `GradientTape` Object | Calculate Gradients | Update Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nowadays people implement neural networks in modern frameworks that are capable of automatic differentiation, such as TensorFlow. \n",
    "\n",
    "Automatic differentiation is implemented with the help of computation graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GradientTape in Tensorflow**\n",
    "\n",
    "The API through which you can leverage TensorFlow’s powerful automatic differentiation capabilities is the GradientTape. \n",
    "\n",
    "It’s a Python scope that will “record” the tensor operations that run inside it, in the form of a computation graph (sometimes called a “tape”). \n",
    "\n",
    "This graph can then be used to retrieve the gradient of any output with respect to any variable or set of variables (instances of the tf.Variable class). \n",
    "\n",
    "A tf.Variable is a specific kind of tensor meant to hold mutable state—for instance, the weights of a neural network are always tf.Variable instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.0>\n",
      "y = tf.Tensor(3.0, shape=(), dtype=float32)\n",
      "gradient = tf.Tensor(2.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a scalar Variable with an initial value of 0 \n",
    "x = tf.Variable(0.)\n",
    "\n",
    "# Open a GradientTape scope\n",
    "with tf.GradientTape() as tape:\n",
    "    # Inside the scope, apply some tensor operations to our variable\n",
    "    y = 2 * x + 3\n",
    "\n",
    "# Use the tape to retrieve the gradient of the output y with respect to our variable x\n",
    "grad_of_y_wrt_x = tape.gradient(y, x)\n",
    "\n",
    "print(\"x =\", x)\n",
    "print(\"y =\", y)\n",
    "print(\"gradient =\", grad_of_y_wrt_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = <tf.Variable 'Variable:0' shape=(2, 2) dtype=float32, numpy=\n",
      "array([[0.89283407, 0.88217425],\n",
      "       [0.9997405 , 0.37762022]], dtype=float32)>\n",
      "y = tf.Tensor(\n",
      "[[4.7856684 4.7643485]\n",
      " [4.999481  3.7552404]], shape=(2, 2), dtype=float32)\n",
      "gradient = tf.Tensor(\n",
      "[[2. 2.]\n",
      " [2. 2.]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# GradientTape works with tensors\n",
    "\n",
    "x = tf.Variable(tf.random.uniform((2, 2)))\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    y = 2 * x + 3\n",
    "\n",
    "grad_of_y_wrt_x = tape.gradient(y, x)\n",
    "\n",
    "print(\"x =\", x)\n",
    "print(\"y =\", y)\n",
    "print(\"gradient =\", grad_of_y_wrt_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W = <tf.Variable 'Variable:0' shape=(2, 2) dtype=float32, numpy=\n",
      "array([[0.5926453 , 0.4134344 ],\n",
      "       [0.03614008, 0.11172056]], dtype=float32)>\n",
      "b = <tf.Variable 'Variable:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)>\n",
      "x = tf.Tensor(\n",
      "[[0.49944055 0.98948157]\n",
      " [0.3720045  0.22848749]], shape=(2, 2), dtype=float32)\n",
      "y = tf.Tensor(\n",
      "[[0.33175105 0.31703132]\n",
      " [0.22872427 0.1793262 ]], shape=(2, 2), dtype=float32)\n",
      "gradient = [<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
      "array([[0.87144506, 0.87144506],\n",
      "       [1.2179691 , 1.2179691 ]], dtype=float32)>, <tf.Tensor: shape=(2,), dtype=float32, numpy=array([2., 2.], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "# GradientTape works with list of variables\n",
    "\n",
    "W = tf.Variable(tf.random.uniform((2, 2)))\n",
    "b = tf.Variable(tf.zeros((2,)))\n",
    "x = tf.random.uniform((2, 2))\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    y = tf.matmul(x, W) + b\n",
    "\n",
    "grad_of_y_wrt_W_and_b = tape.gradient(y, [W, b])\n",
    "\n",
    "print(\"W =\", W)\n",
    "print(\"b =\", b)\n",
    "print(\"x =\", x)\n",
    "print(\"y =\", y)\n",
    "print(\"gradient =\", grad_of_y_wrt_W_and_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training Function - One Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_training_step(model, images_batch, labels_batch):\n",
    "    # Open a GradientTape scope\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Predicting on the batch of data using the model\n",
    "        predictions = model(images_batch)\n",
    "\n",
    "        # list of loss per sample for the complete batch\n",
    "        per_sample_losses = tf.keras.losses.sparse_categorical_crossentropy(labels_batch, predictions)\n",
    "        \n",
    "        # Calculating the average loss for that complete batch\n",
    "        average_loss = tf.reduce_mean(per_sample_losses)\n",
    "\n",
    "    # Compute the gradient of the loss with regard to the weights\n",
    "    # The output gradients is a list where each entry corresponds to a weight from the model.weights list\n",
    "    gradients = tape.gradient(average_loss, model.weights)\n",
    "    \n",
    "    # Function to update weights (defined below)\n",
    "    update_weights(gradients, model.weights)\n",
    "\n",
    "    # Returning average_loss\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `update_weights` Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The “weight update” step (represented by the preceding update_weights function) is to move the weights by “a bit” in a direction that will reduce the loss on this batch. \n",
    "\n",
    "The magnitude of the move is determined by the “learning rate,” typically a small quantity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "\n",
    "def update_weights(gradients, weights):\n",
    "    for g, w in zip(gradients, weights):\n",
    "        # equivalent to -= operation\n",
    "        w.assign_sub(g * learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice we never implement a weight update step like this.\n",
    "\n",
    "Instead, we would use an Optimizer instance from Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the optimizer object\n",
    "optimizer = optimizers.SGD(learning_rate=1e-3)\n",
    "\n",
    "# Redefining the function with the optimizer object\n",
    "def update_weights(gradients, weights):\n",
    "    optimizer.apply_gradients(zip(gradients, weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit Function - Full Training Loop Using One Traing Step Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, images, labels, epochs, batch_size=128):\n",
    "    # Loop will run epoch times\n",
    "    for epoch_counter in range(epochs):\n",
    "        # Printing epoch numbers\n",
    "        print(f\"Epoch {epoch_counter}\")\n",
    "\n",
    "        # Instantiating BatchGenerator object\n",
    "        batch_generator = BatchGenerator(images, labels)\n",
    "\n",
    "        # Loop will run num_batches times\n",
    "        # num_batches required will be calculated based on the size of the training dataset and batch_size\n",
    "        for batch_counter in range(batch_generator.num_batches):\n",
    "            # Getting one batch at a time until going through all the batches (num_batches times)\n",
    "            images_batch, labels_batch = batch_generator.next()\n",
    "            # getting average loss for that batch and adjusting weights as well\n",
    "            loss = one_training_step(model, images_batch, labels_batch)\n",
    "            # Printing loss per batch\n",
    "            if batch_counter % 100 == 0:\n",
    "                print(f\"loss at batch {batch_counter}: {loss:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Drive of the Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading MNIST data\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split and Scale Data\n",
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype(\"float32\") / 255\n",
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "test_images = test_images.astype(\"float32\") / 255 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "loss at batch 0: 4.66\n",
      "loss at batch 100: 2.25\n",
      "loss at batch 200: 2.25\n",
      "loss at batch 300: 2.12\n",
      "loss at batch 400: 2.28\n",
      "Epoch 1\n",
      "loss at batch 0: 1.94\n",
      "loss at batch 100: 1.89\n",
      "loss at batch 200: 1.88\n",
      "loss at batch 300: 1.73\n",
      "loss at batch 400: 1.89\n",
      "Epoch 2\n",
      "loss at batch 0: 1.61\n",
      "loss at batch 100: 1.59\n",
      "loss at batch 200: 1.54\n",
      "loss at batch 300: 1.44\n",
      "loss at batch 400: 1.56\n",
      "Epoch 3\n",
      "loss at batch 0: 1.35\n",
      "loss at batch 100: 1.35\n",
      "loss at batch 200: 1.27\n",
      "loss at batch 300: 1.22\n",
      "loss at batch 400: 1.31\n",
      "Epoch 4\n",
      "loss at batch 0: 1.14\n",
      "loss at batch 100: 1.16\n",
      "loss at batch 200: 1.06\n",
      "loss at batch 300: 1.05\n",
      "loss at batch 400: 1.14\n",
      "Epoch 5\n",
      "loss at batch 0: 0.99\n",
      "loss at batch 100: 1.02\n",
      "loss at batch 200: 0.92\n",
      "loss at batch 300: 0.93\n",
      "loss at batch 400: 1.01\n",
      "Epoch 6\n",
      "loss at batch 0: 0.88\n",
      "loss at batch 100: 0.91\n",
      "loss at batch 200: 0.81\n",
      "loss at batch 300: 0.84\n",
      "loss at batch 400: 0.92\n",
      "Epoch 7\n",
      "loss at batch 0: 0.80\n",
      "loss at batch 100: 0.82\n",
      "loss at batch 200: 0.73\n",
      "loss at batch 300: 0.77\n",
      "loss at batch 400: 0.85\n",
      "Epoch 8\n",
      "loss at batch 0: 0.73\n",
      "loss at batch 100: 0.76\n",
      "loss at batch 200: 0.67\n",
      "loss at batch 300: 0.71\n",
      "loss at batch 400: 0.79\n",
      "Epoch 9\n",
      "loss at batch 0: 0.68\n",
      "loss at batch 100: 0.70\n",
      "loss at batch 200: 0.62\n",
      "loss at batch 300: 0.66\n",
      "loss at batch 400: 0.75\n"
     ]
    }
   ],
   "source": [
    "# Fit the data in the model\n",
    "fit(model, train_images, train_labels, epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape (10000, 10)\n",
      "First 2 Prediction as Tensorflow Variable/Tensor\n",
      " tf.Tensor(\n",
      "[[1.2674748e-02 1.5277988e-03 4.2626788e-03 1.5320774e-02 1.8537574e-02\n",
      "  7.2525530e-03 2.5645422e-03 8.4755456e-01 8.8109467e-03 8.1493877e-02]\n",
      " [6.8721779e-02 2.1453366e-02 3.0072197e-01 2.7287203e-01 2.3117899e-03\n",
      "  7.3553540e-02 1.8272300e-01 5.7506381e-04 7.4266292e-02 2.8011312e-03]], shape=(2, 10), dtype=float32)\n",
      "\n",
      "Shape (10000, 10)\n",
      "First 2 Prediction as Numpy Tensor\n",
      " [[1.2674748e-02 1.5277988e-03 4.2626788e-03 1.5320774e-02 1.8537574e-02\n",
      "  7.2525530e-03 2.5645422e-03 8.4755456e-01 8.8109467e-03 8.1493877e-02]\n",
      " [6.8721779e-02 2.1453366e-02 3.0072197e-01 2.7287203e-01 2.3117899e-03\n",
      "  7.3553540e-02 1.8272300e-01 5.7506381e-04 7.4266292e-02 2.8011312e-03]]\n",
      "\n",
      "Shape (10000,)\n",
      "First 2 Prediction as Numpy Tensor\n",
      " [7 2]\n",
      "\n",
      "Shape (10000,)\n",
      "First 2 Prediction as Numpy Tensor\n",
      " [ True  True]\n",
      "\n",
      "accuracy: 0.82\n"
     ]
    }
   ],
   "source": [
    "# Making the prediction using the model\n",
    "predictions = model(test_images)\n",
    "print(\"Shape\", predictions.shape)\n",
    "print(\"First 2 Prediction as Tensorflow Variable/Tensor\\n\", predictions[0:2])\n",
    "\n",
    "print()\n",
    "\n",
    "# Converting Tensorflow Variable to Numpy array\n",
    "predictions = predictions.numpy()\n",
    "print(\"Shape\", predictions.shape)\n",
    "print(\"First 2 Prediction as Numpy Tensor\\n\", predictions[0:2])\n",
    "\n",
    "print()\n",
    "\n",
    "# Getting the indices of the highest probabilities from the list of predictions\n",
    "# This indices will work as class labels\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "print(\"Shape\", predicted_labels.shape)\n",
    "print(\"First 2 Prediction as Numpy Tensor\\n\", predicted_labels[0:2])\n",
    "\n",
    "print()\n",
    "\n",
    "# Creating a binary array comparing the predicted labels and actual labels for all the test samples\n",
    "matches = predicted_labels == test_labels\n",
    "print(\"Shape\", matches.shape)\n",
    "print(\"First 2 Prediction as Numpy Tensor\\n\", matches[0:2])\n",
    "\n",
    "print()\n",
    "\n",
    "# Calculating the mean (True/(True+False))\n",
    "print(f\"accuracy: {matches.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
